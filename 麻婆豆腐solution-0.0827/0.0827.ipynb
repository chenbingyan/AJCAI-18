{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 时间处理\n",
    "def time2cov(time_):\n",
    "    '''\n",
    "    时间是根据天数推移，所以日期为脱敏，但是时间本身不脱敏\n",
    "    :param time_:\n",
    "    :return:\n",
    "    '''\n",
    "    return time.strftime(\"%Y-%m-%d %H:%M:%S\",time.localtime(time_))\n",
    "\n",
    "print('train')\n",
    "train = pd.read_csv('../data/round1_ijcai_18_train_20180301.txt',sep=\" \")\n",
    "train = train.drop_duplicates(['instance_id'])\n",
    "train = train.reset_index(drop=True)\n",
    "\n",
    "print('test')\n",
    "test_a = pd.read_csv('../data/round1_ijcai_18_test_a_20180301.txt',sep=\" \")\n",
    "\n",
    "all_data = pd.concat([train,test_a])\n",
    "all_data['real_time'] = pd.to_datetime(all_data['context_timestamp'].apply(time2cov))\n",
    "all_data['real_hour'] = all_data['real_time'].dt.hour\n",
    "all_data['real_day'] = all_data['real_time'].dt.day\n",
    "\n",
    "def time_change(hour):\n",
    "    hour = hour - 1\n",
    "    if hour == -1:\n",
    "        hour = 23\n",
    "    return hour\n",
    "\n",
    "def time_change_1(hour):\n",
    "    hour = hour + 1\n",
    "    if hour == 24:\n",
    "        hour = 0\n",
    "    return hour\n",
    "\n",
    "all_data['hour_before'] = all_data['real_hour'].apply(time_change)\n",
    "all_data['hour_after'] = all_data['real_hour'].apply(time_change_1)\n",
    "\n",
    "# 18 21 19 20 22 23 24 | 25\n",
    "print(all_data['real_day'].unique())\n",
    "\n",
    "# train and test cov radio\n",
    "# print(len((set(train['user_id']))&(set(test_a['user_id'])))/len(set(test_a['user_id'])))\n",
    "# print(len((set(train['shop_id']))&(set(test_a['shop_id'])))/len(set(test_a['shop_id'])))\n",
    "# print(len((set(train['item_id']))&(set(test_a['item_id'])))/len(set(test_a['item_id'])))\n",
    "# user 0.26714801444043323\n",
    "# shop 0.9781637717121588\n",
    "# item 0.956427604871448\n",
    "\n",
    "# shop feat\n",
    "\n",
    "# item feat\n",
    "\n",
    "# user feat\n",
    "def c_log_loss(y_t,y_p):\n",
    "    tmp = np.array(y_t) * np.log(np.array(y_p)) + (1 - np.array(y_t)) * np.log(1 - np.array(y_p))\n",
    "    return -np.sum(tmp)/len(y_t),False\n",
    "\n",
    "# 获取当前时间之前的前x天的转化率特征\n",
    "def get_before_cov_radio(all_data,label_data,cov_list = list(['shop_id','item_id','real_hour','item_pv_level','item_sales_level']),day_list = list([1,2,3])):\n",
    "    result = []\n",
    "    r = pd.DataFrame()\n",
    "    label_data_time = label_data['real_day'].min()\n",
    "    label_data_time_set = label_data['real_day'].unique()\n",
    "    print('label set day',label_data_time_set)\n",
    "    for cov in cov_list:\n",
    "        for d in day_list:\n",
    "            feat_set = all_data[\n",
    "                (all_data['real_day']>=label_data_time-d)&(all_data['real_day']<label_data_time)\n",
    "                                ]\n",
    "            print(\"cov feature\",feat_set['real_day'].unique())\n",
    "            print(\"cov time\",cov)\n",
    "\n",
    "            tmp = feat_set.groupby([cov],as_index=False).is_trade.agg({'mean':np.mean,'count':'count'}).add_suffix(\"_%s_before_%d_day\"%(cov,d))\n",
    "\n",
    "            tmp.rename(columns={'%s_%s_before_%d_day'%(cov,cov,d):cov},inplace=True)\n",
    "\n",
    "            if d == 1:\n",
    "                r = tmp\n",
    "            else:\n",
    "                r = pd.merge(r,tmp,on=[cov],how='outer').fillna(0)\n",
    "\n",
    "        result.append(r)\n",
    "    return result\n",
    "\n",
    "\n",
    "def calc_categry_feat(data):\n",
    "    data['item_category_list_1'] = data['item_category_list'].apply(lambda x: int(x.split(';')[0]))\n",
    "    data['item_category_list_2'] = data['item_category_list'].apply(lambda x: int(x.split(';')[1]))\n",
    "    data['item_property_list_0'] = data['item_property_list'].apply(lambda x: int(x.split(';')[0]))\n",
    "    data['item_property_list_1'] = data['item_property_list'].apply(lambda x: int(x.split(';')[1]))\n",
    "    data['item_property_list_2'] = data['item_property_list'].apply(lambda x: int(x.split(';')[2]))\n",
    "\n",
    "    for i in range(3):\n",
    "        data['predict_category_%d' % (i)] = data['predict_category_property'].apply(\n",
    "            lambda x: int(str(x.split(\";\")[i]).split(\":\")[0]) if len(x.split(\";\")) > i else -1\n",
    "        )\n",
    "\n",
    "    for item_cate in ['item_category_list_1','item_category_list_2']:\n",
    "        for pre_item_cate in ['predict_category_0','predict_category_1','predict_category_2']:\n",
    "            data['%s_%s'%(item_cate,pre_item_cate)] = data[item_cate] == data[pre_item_cate]\n",
    "            data['%s_%s'%(item_cate,pre_item_cate)] = data['%s_%s'%(item_cate,pre_item_cate)].astype(int)\n",
    "\n",
    "    del data['item_category_list']\n",
    "    del data['item_property_list']\n",
    "    del data['predict_category_property']\n",
    "    return data\n",
    "\n",
    "take_columns = ['instance_id','item_id','shop_id','user_id','is_trade']\n",
    "\n",
    "shop_current_col = [\n",
    "    'shop_score_description','shop_score_delivery','shop_score_service',\n",
    "    'shop_star_level','shop_review_positive_rate','shop_review_num_level'\n",
    "]\n",
    "\n",
    "user_col = [\n",
    "    'user_gender_id','user_age_level','user_occupation_id','user_star_level'\n",
    "]\n",
    "\n",
    "item_col = [\n",
    "    'item_brand_id','item_city_id','item_price_level',\n",
    "    'item_sales_level','item_collected_level','item_pv_level',\n",
    "    'item_category_list','item_property_list'\n",
    "]\n",
    "time_feat = ['real_hour','hour_before','hour_after','context_timestamp','real_day']\n",
    "\n",
    "context_col = ['predict_category_property','context_page_id']\n",
    "\n",
    "feat = take_columns + shop_current_col + time_feat + user_col + item_col + context_col\n",
    "\n",
    "def get_history_user_feat(all_data,data):\n",
    "    label_data_time = data['real_day'].min()\n",
    "    print(label_data_time)\n",
    "\n",
    "    tmp = all_data[all_data['real_day'] < label_data_time]\n",
    "    print(tmp['real_day'].unique())\n",
    "\n",
    "    user_time = tmp.groupby(['user_id'],as_index=False).context_timestamp.agg({'day_begin':'min','day_end':'max'})\n",
    "    user_time['alive'] = user_time['day_end'] - user_time['day_begin']\n",
    "    user_time['s_alive'] = label_data_time - user_time['day_begin']\n",
    "    user_time['alive/s_alive'] =  user_time['alive'] / user_time['s_alive']\n",
    "\n",
    "    user_time_cov = tmp[tmp['is_trade']==1]\n",
    "    user_time_cov = user_time_cov.groupby(['user_id'], as_index=False).context_timestamp.agg({'day_end_cov': 'max'})\n",
    "\n",
    "    user_time_cov = pd.DataFrame(user_time_cov).drop_duplicates(['user_id','day_end_cov'])\n",
    "\n",
    "    data = pd.merge(data,user_time[['user_id','alive','s_alive','alive/s_alive','day_begin','day_end']],on=['user_id'],how='left')\n",
    "\n",
    "    data = pd.merge(data,user_time_cov,on=['user_id'],how='left')\n",
    "    data['day_end_cov'] = data['day_end_cov'].fillna(data['day_end'])\n",
    "\n",
    "    data['alive_cov'] = data['day_end_cov'] - data['day_begin']\n",
    "    data['alive/alive_cov'] = data['alive'] / data['alive_cov']\n",
    "    # data['s_alive/alive_cov'] = data['s_alive'] / data['alive_cov']\n",
    "\n",
    "    del data['day_end_cov']\n",
    "    del data['day_end']\n",
    "    del data['day_begin']\n",
    "\n",
    "    # for i in [1,2,3]:\n",
    "    #     tmp = all_data[(all_data['real_day'] < data['real_day'].min()) & (all_data['real_day'] >= data['real_day'].min() - i)]\n",
    "    #     user_item_sales_level_day = tmp.groupby(['user_id'], as_index=False)['item_sales_level'] \\\n",
    "    #         .agg({'user_item_sales_level_day_mean': 'mean',\n",
    "    #               'user_item_sales_level_day_median': 'median',\n",
    "    #               'user_item_sales_level_day_min': 'min',\n",
    "    #               'user_item_sales_level_day_max': 'max',\n",
    "    #               'user_item_sales_level_day_std': 'std',\n",
    "    #               'user_item_sales_level_day_count': 'count'})\n",
    "    #     data = pd.merge(data, user_item_sales_level_day, 'left', on=['user_id'])\n",
    "\n",
    "    # data = data[['user_id','alive','s_alive','alive/s_alive','alive_cov','alive/alive_cov']]\n",
    "\n",
    "    return data.fillna(-1)\n",
    "\n",
    "\n",
    "def get_history_shop_feat(all_data,data):\n",
    "    label_data_time = data['real_day'].min()\n",
    "    print(label_data_time)\n",
    "    for i in [1,2,3]:\n",
    "        tmp = all_data[(all_data['real_day'] < label_data_time)&(all_data['real_day'] >= label_data_time - i)]\n",
    "\n",
    "        shop_score_service_hour = tmp.groupby(['real_hour'], as_index=False)[\n",
    "            'shop_score_service'] \\\n",
    "            .agg({\n",
    "                  'shop_score_service_hour_std_%d'%(i): 'std',\n",
    "               })\n",
    "        data = pd.merge(data, shop_score_service_hour, 'left', on=['real_hour'])\n",
    "\n",
    "        shop_score_delivery = tmp.groupby(['real_hour'], as_index=False)[\n",
    "            'shop_score_delivery'] \\\n",
    "            .agg({\n",
    "            'shop_score_delivery_hour_std_%d' % (i): 'std',\n",
    "        })\n",
    "        data = pd.merge(data, shop_score_delivery, 'left', on=['real_hour'])\n",
    "\n",
    "        shop_score_service_hour = tmp.groupby(['real_hour'], as_index=False)[\n",
    "            'shop_score_description'] \\\n",
    "            .agg({\n",
    "            'shop_score_description_hour_std_%d' % (i): 'std',\n",
    "        })\n",
    "        data = pd.merge(data, shop_score_service_hour, 'left', on=['real_hour'])\n",
    "\n",
    "        shop_review_positive_rate = tmp.groupby(['real_hour'], as_index=False)[\n",
    "            'shop_review_positive_rate'] \\\n",
    "            .agg({\n",
    "            'shop_review_positive_rate_hour_std_%d' % (i): 'std',\n",
    "        })\n",
    "        data = pd.merge(data, shop_review_positive_rate, 'left', on=['real_hour'])\n",
    "\n",
    "        shop_star_level = tmp.groupby(['real_hour'], as_index=False)[\n",
    "            'shop_star_level'] \\\n",
    "            .agg({\n",
    "            'shop_star_level_hour_std_%d' % (i): 'std',\n",
    "        })\n",
    "        data = pd.merge(data, shop_star_level, 'left', on=['real_hour'])\n",
    "\n",
    "        shop_review_num_level = tmp.groupby(['real_hour'], as_index=False)[\n",
    "            'shop_review_num_level'] \\\n",
    "            .agg({\n",
    "            'shop_review_num_level_hour_std_%d' % (i): 'std',\n",
    "        })\n",
    "        data = pd.merge(data, shop_review_num_level, 'left', on=['real_hour'])\n",
    "\n",
    "        shop_query_day_hour = tmp.groupby(['shop_id', 'real_hour']).size().reset_index().rename(\n",
    "            columns={0: 'shop_query_day_hour_%d'%(i)})\n",
    "        data = pd.merge(data, shop_query_day_hour, 'left', on=['shop_id', 'real_hour'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_history_item_feat(all_data,data):\n",
    "    for i in [1, 2, 3]:\n",
    "        tmp = all_data[(all_data['real_day']<data['real_day'].min())&(all_data['real_day']>=data['real_day'].min()-i)]\n",
    "\n",
    "        item_brand_id_day = tmp.groupby(['item_city_id','real_hour']).size().reset_index().rename(\n",
    "            columns={0: 'item_brand_id_day_%d'%(i)})\n",
    "        data = pd.merge(data, item_brand_id_day, 'left', on=['item_city_id','real_hour'])\n",
    "\n",
    "        item_brand_id_hour = tmp.groupby(['item_brand_id', 'real_hour']).size().reset_index().rename(\n",
    "            columns={0: 'item_brand_id_hour_%d'%(i)})\n",
    "        data = pd.merge(data, item_brand_id_hour, 'left', on=['item_brand_id', 'real_hour'])\n",
    "        item_pv_level_hour = tmp.groupby(['item_pv_level', 'real_hour']).size().reset_index().rename(\n",
    "            columns={0: 'item_pv_level_hour_%d'%(i)})\n",
    "        data = pd.merge(data, item_pv_level_hour, 'left', on=['item_pv_level','real_hour'])\n",
    "        #\n",
    "        # item_pv_level_day = data.groupby(['real_day','real_hour'], as_index=False)['item_pv_level'] \\\n",
    "        #     .agg({'item_pv_level_day_mean_%d'%(i): 'mean',\n",
    "        #           'item_pv_level_day_median_%d'%(i): 'median',\n",
    "        #           'item_pv_level_day_std_%d'%(i): 'std'\n",
    "        #           })\n",
    "        # data = pd.merge(data, item_pv_level_day, 'left', on=['real_day','real_hour'])\n",
    "    return data\n",
    "\n",
    "print('make feat')\n",
    "def make_feat(data,feat):\n",
    "    '''\n",
    "    :param data: 标签数据，当前时刻的用户特征\n",
    "    :param feat: 特征数据，统计的用户特征\n",
    "    :return: 拼接后的特征\n",
    "    '''\n",
    "\n",
    "    data = calc_categry_feat(data)\n",
    "    data = get_history_user_feat(all_data,data)\n",
    "    data = get_history_shop_feat(all_data,data)\n",
    "    data = get_history_item_feat(all_data,data)\n",
    "\n",
    "    for f in feat:\n",
    "        data = pd.merge(data,f,on=[f.columns[0]],how='left')\n",
    "\n",
    "    return data.fillna(0)\n",
    "\n",
    "test_a = all_data[train.shape[0]:]\n",
    "\n",
    "train = all_data[:train.shape[0]]\n",
    "val_a = train[train['real_day']==24]\n",
    "train_a = train[train['real_day']==23]\n",
    "train_b = train[train['real_day']==22]\n",
    "train_c = train[train['real_day']==21]\n",
    "\n",
    "# 传入全部数据和当前标签数据\n",
    "test_cov_feat = get_before_cov_radio(all_data,test_a)\n",
    "val_cov_feat = get_before_cov_radio(all_data,val_a)\n",
    "\n",
    "train_cov_feat_a = get_before_cov_radio(all_data,train_a)\n",
    "train_cov_feat_b = get_before_cov_radio(all_data,train_b)\n",
    "train_cov_feat_c = get_before_cov_radio(all_data,train_c)\n",
    "\n",
    "\n",
    "train_a = make_feat(train_a[feat],train_cov_feat_a)\n",
    "train_b = make_feat(train_b[feat],train_cov_feat_b)\n",
    "train_c = make_feat(train_c[feat],train_cov_feat_c)\n",
    "\n",
    "test_a = make_feat(test_a[feat],test_cov_feat)\n",
    "val_a = make_feat(val_a[feat],val_cov_feat)\n",
    "\n",
    "\n",
    "train = pd.concat([train_a,train_b])\n",
    "train = pd.concat([train,train_c])\n",
    "\n",
    "# print(train.shape)\n",
    "# train = pd.concat([train,val_a])\n",
    "# print(train.shape)\n",
    "\n",
    "y_train = train.pop('is_trade')\n",
    "train_index = train.pop('instance_id')\n",
    "X_train = train\n",
    "\n",
    "y_test = test_a.pop('is_trade')\n",
    "test_index = test_a.pop('instance_id')\n",
    "X_test = test_a\n",
    "\n",
    "y_val = val_a.pop('is_trade')\n",
    "val_index = val_a.pop('instance_id')\n",
    "X_val = val_a\n",
    "\n",
    "\n",
    "# print(train.head())\n",
    "\n",
    "category_list = [\n",
    "    'item_id','shop_id','user_id','user_gender_id','user_age_level',\n",
    "     'user_occupation_id','user_star_level',\n",
    "    'item_brand_id', 'item_city_id', 'item_price_level',\n",
    "    'item_sales_level', 'item_collected_level', 'item_pv_level',\n",
    "    'shop_review_num_level','shop_star_level','item_category_list_1','item_category_list_2',\n",
    "    'item_property_list_0','item_property_list_1','item_property_list_2',\n",
    "    'predict_category_0','predict_category_1','predict_category_2','context_page_id'\n",
    "]\n",
    "\n",
    "def make_cat(data):\n",
    "    for i in category_list:\n",
    "        data[i] = data[i].astype('category')\n",
    "    return data\n",
    "\n",
    "\n",
    "train_test_val = pd.concat([X_train,X_test])\n",
    "train_test_val = pd.concat([train_test_val,X_val])\n",
    "train_test_val = train_test_val.reset_index(drop=True)\n",
    "\n",
    "# train_test_val = make_cat(train_test_val)\n",
    "#\n",
    "# X_train = train_test_val[:X_train.shape[0]]\n",
    "# X_test = train_test_val[X_train.shape[0]:X_train.shape[0]+X_test.shape[0]]\n",
    "# X_val = train_test_val[X_train.shape[0]+X_test.shape[0]:]\n",
    "\n",
    "X_train = make_cat(X_train)\n",
    "X_test = make_cat(X_test)\n",
    "X_val = make_cat(X_val)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "\n",
    "# X_test = make_cat(X_test)\n",
    "# X_val = make_cat(X_val)\n",
    "\n",
    "del X_train['hour_before']\n",
    "del X_test['hour_before']\n",
    "del X_val['hour_before']\n",
    "\n",
    "del X_train['hour_after']\n",
    "del X_test['hour_after']\n",
    "del X_val['hour_after']\n",
    "\n",
    "del X_train['real_day']\n",
    "del X_test['real_day']\n",
    "del X_val['real_day']\n",
    "\n",
    "print(X_train.dtypes)\n",
    "\n",
    "del X_train['context_timestamp']\n",
    "del X_test['context_timestamp']\n",
    "del X_val['context_timestamp']\n",
    "\n",
    "X_train = X_train[X_train.columns]\n",
    "X_test = X_test[X_train.columns]\n",
    "X_val = X_val[X_train.columns]\n",
    "\n",
    "import lightgbm as lgb\n",
    "#\n",
    "# 线下学习\n",
    "gbm = lgb.LGBMRegressor(objective='binary',\n",
    "                        num_leaves=32,\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators=2000,\n",
    "                        colsample_bytree = 0.65,\n",
    "                        subsample = 0.65,\n",
    "                        seed=0\n",
    "                        )\n",
    "gbm.fit(X_train,y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['binary_logloss'],\n",
    "    early_stopping_rounds= 200)\n",
    "\n",
    "imp = pd.DataFrame()\n",
    "imp['n'] = list(X_train.columns)\n",
    "imp['s'] = list(gbm.feature_importances_)\n",
    "print(imp.sort_values('s',ascending=False))\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict\n",
    "y_pred_1 = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
    "y_tt = gbm.predict(X_train, num_iteration=gbm.best_iteration)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "print(log_loss(y_val,y_pred_1))\n",
    "\n",
    "print(log_loss(y_train,y_tt))\n",
    "\n",
    "# 线上提交\n",
    "\n",
    "gbm_sub = lgb.LGBMRegressor(objective='binary',\n",
    "                        num_leaves=32,\n",
    "                        learning_rate=0.01,\n",
    "                        n_estimators = gbm.best_iteration+1,\n",
    "                        colsample_bytree = 0.65,\n",
    "                        subsample = 0.65,\n",
    "                        seed=0\n",
    "                        )\n",
    "\n",
    "X_train = pd.concat([X_train,X_val])\n",
    "y_train = pd.concat([y_train,y_val])\n",
    "\n",
    "X_train = make_cat(X_train)\n",
    "\n",
    "X_train = X_train[X_train.columns]\n",
    "\n",
    "gbm_sub.fit(X_train,y_train,\n",
    "    eval_set=[(X_train, y_train)],\n",
    "    eval_metric=['binary_logloss'])\n",
    "\n",
    "y_sub_1 = gbm_sub.predict(X_test)\n",
    "\n",
    "y_tt = gbm_sub.predict(X_train, num_iteration=gbm_sub.best_iteration)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "print(log_loss(y_train,y_tt))\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['instance_id'] = list(test_index)\n",
    "\n",
    "sub['predicted_score'] = list(y_sub_1)\n",
    " \n",
    "sub.to_csv('../result/20180409.txt',sep=\" \",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
